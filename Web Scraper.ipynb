{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1022fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "import time\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4694b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = pd.read_excel(\"Website Samples.xlsx\").to_numpy().flatten()\n",
    "\n",
    "#vars associated with Chrome\n",
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d86193",
   "metadata": {},
   "source": [
    "# This code helps us get the wayback machine URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wayback(url, timestamp):\n",
    "    url = f\"http://archive.org/wayback/available?url={url}&timestamp={timestamp}\"\n",
    "    print(\"GET_WAYBACK: \" + url)\n",
    "    res = requests.get(url).json()\n",
    "     \n",
    "    if(len(res['archived_snapshots']) == 0):\n",
    "        return \"\"\n",
    "    \n",
    "    return res['archived_snapshots']['closest']['url']\n",
    "\n",
    "\n",
    "def gen_url(urls):\n",
    "    current_date = datetime.now()\n",
    "    start_date_1_year_ago = current_date - timedelta(days=1*365)\n",
    "\n",
    "    first_url, second_url = \"\", \"\"\n",
    "\n",
    "    i1, i2 = 0, 0\n",
    "\n",
    "    first_random_date = None\n",
    "    second_random_date = None; \n",
    "\n",
    "\n",
    "    while (i1 <= 10 and first_url == \"\"):\n",
    "        first_random_date = generate_random_date(start_date_1_year_ago, current_date)\n",
    "        first_url = get_wayback(urls, first_random_date.strftime('%Y%m%d'))\n",
    "        i1 = i1 + 1\n",
    "\n",
    "\n",
    "\n",
    "    while(i2 <= 10 and second_url == \"\" and first_url != \"\"):\n",
    "        start_date_before_first_date = first_random_date - timedelta(days=(1.5*364))\n",
    "        end_date_before_first_date = first_random_date - timedelta(days=0.5*364)\n",
    "        second_random_date = generate_random_date(start_date_before_first_date, end_date_before_first_date)\n",
    "\n",
    "        second_url = get_wayback(urls, second_random_date.strftime('%Y%m%d'))\n",
    "        i2 = i2 + 1\n",
    "        \n",
    "    if(first_url != \"\" and second_url != \"\"):\n",
    "        return [first_url, second_url, first_random_date.strftime('%Y%m%d'), second_random_date.strftime('%Y%m%d')]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181bf36",
   "metadata": {},
   "source": [
    "# The code belows allows us to determine if we can scrape from a website based on robots.txt (lots of webpages have JS protections on them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scrapeable(url):\n",
    "    # Parse the URL to get the base URL\n",
    "    parsed_url = urlparse(url)\n",
    "    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "    # Create a RobotFileParser object\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "    robots_url = urljoin(base_url, \"robots.txt\")  # Construct the robots.txt URL\n",
    "    robot_parser.set_url(robots_url)\n",
    "    robot_parser.read()\n",
    "\n",
    "    # Check if the URL is allowed to be scraped\n",
    "    return robot_parser.can_fetch(\"*\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_scrapeable(\"https://www.google.com/search/about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e59b16",
   "metadata": {},
   "source": [
    "# This code below helps us randomize dates/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5526309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_date(start_date, end_date):\n",
    "    time_difference = end_date - start_date\n",
    "    random_days = random.randint(0, time_difference.days)\n",
    "    random_date = start_date + timedelta(days=random_days)\n",
    "    return random_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c885d1",
   "metadata": {},
   "source": [
    "# This code is what gets the HTML and saves it to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfafcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_html_with_id(url, name, unique_id):\n",
    "    # Create a folder name based on the URL\n",
    "    folder_name = \"./Data/\" + name\n",
    "    \n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "       \n",
    "    # options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Create a filename with the URL and ID\n",
    "    filename = f\"{unique_id}.html\"\n",
    "    \n",
    "    # Save the HTML content to the generated filename\n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as html_file:\n",
    "        html_file.write(str(soup))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    print(f\"SAVE_HTML: HTML page saved as '{filename}' in '{folder_name}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540a5f5",
   "metadata": {},
   "source": [
    "# Actual Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url in web_url:\n",
    "    print(\"Processing \" + url)\n",
    "    url_arr = gen_url(url)\n",
    "    \n",
    "    if(url_arr == None):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    print(f\"URL Arr: {url_arr}\\n\")\n",
    "    \n",
    "    \n",
    "    save_html_with_id(url_arr[0], url.split(\".\")[0], \"new\")\n",
    "    save_html_with_id(url_arr[1], url.split(\".\")[0], \"old\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
